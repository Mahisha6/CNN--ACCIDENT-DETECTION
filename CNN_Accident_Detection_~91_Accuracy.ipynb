{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr4tAS7KvNFY",
        "outputId": "0ac8ceae-a9c2-440c-e9c3-659016c10532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/804753/1379553/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240506%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240506T114926Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=7ecae602c695d6596a342525a82766f308cf7044e27f643484ae05ad71d2c398e765ac03bbb3603246d9e216987c032a016e1d01c96f08b0ea9805e7396beaf9a40f4b10c62a7fe61d2d69690a96ea852aabe2da298be8769a994582cdaadb3bf315b10fa36bd98ade2ce23718aab6ff79412ff5c8981513b13113f7aab052e5a68bebeb0a94eec2758bae6e25d8a5079c005b4ace05b2980c2c1d5c2ec9081d8ac29c481aeee8a6d3e31bc18d8ad12d84e9e64318c9a379bdfe06ffcb597231056da7da85d2f4c10f134e1b69ee86a9f84c0aa39661f279ce2e241b524f5c62cd879d59836e277bc90629ab16e9d346794419fae24f6bef8e6399314714d8ba to path /kaggle/input/accident-detection-from-cctv-footage\n",
            "Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/2907015/5009945/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240506%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240506T114926Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=47dcb980fc5a42c8e7fdc36f89849506696fb6219520acaa1f7126412417105b6df91095bed32e95517ab208528d880985dc6311b633c2968a205707d4f9530a08d64eec5213b07eb0958a1d009899d470ad2c05c4e78298fc1cd76375bc975cacfbd60e998d13266a8f33f065455d7b8aa193664c49b88c3b1a4e8853b2a7594bb8178304846540301a5701e776f29febc56856edbe86d6a22324dad311a9eace78c13dde433039d60b209a8db66feafd046f417b3ba571968aeb0b8563f66bee81b67ffb111cbb3f9e61263e94f50fbc48bf0a73060d40487e3db539632e961393673883e1d830a2403d1dc301f19805876cbe1bf775e0718e4c2913573d10 to path /kaggle/input/cctvfootagevideo\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'accident-detection-from-cctv-footage:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F804753%2F1379553%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240506%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240506T114926Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7ecae602c695d6596a342525a82766f308cf7044e27f643484ae05ad71d2c398e765ac03bbb3603246d9e216987c032a016e1d01c96f08b0ea9805e7396beaf9a40f4b10c62a7fe61d2d69690a96ea852aabe2da298be8769a994582cdaadb3bf315b10fa36bd98ade2ce23718aab6ff79412ff5c8981513b13113f7aab052e5a68bebeb0a94eec2758bae6e25d8a5079c005b4ace05b2980c2c1d5c2ec9081d8ac29c481aeee8a6d3e31bc18d8ad12d84e9e64318c9a379bdfe06ffcb597231056da7da85d2f4c10f134e1b69ee86a9f84c0aa39661f279ce2e241b524f5c62cd879d59836e277bc90629ab16e9d346794419fae24f6bef8e6399314714d8ba,cctvfootagevideo:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2907015%2F5009945%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240506%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240506T114926Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D47dcb980fc5a42c8e7fdc36f89849506696fb6219520acaa1f7126412417105b6df91095bed32e95517ab208528d880985dc6311b633c2968a205707d4f9530a08d64eec5213b07eb0958a1d009899d470ad2c05c4e78298fc1cd76375bc975cacfbd60e998d13266a8f33f065455d7b8aa193664c49b88c3b1a4e8853b2a7594bb8178304846540301a5701e776f29febc56856edbe86d6a22324dad311a9eace78c13dde433039d60b209a8db66feafd046f417b3ba571968aeb0b8563f66bee81b67ffb111cbb3f9e61263e94f50fbc48bf0a73060d40487e3db539632e961393673883e1d830a2403d1dc301f19805876cbe1bf775e0718e4c2913573d10'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jYD_us8vNFa"
      },
      "source": [
        "# Setting up our Data  \n",
        "Before we begin with creating and training our model, we will first set the size of the batches for our training, as well as the image height and width to set for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:54:41.199676Z",
          "iopub.status.busy": "2023-04-05T11:54:41.19896Z",
          "iopub.status.idle": "2023-04-05T11:54:41.207234Z",
          "shell.execute_reply": "2023-04-05T11:54:41.206092Z",
          "shell.execute_reply.started": "2023-04-05T11:54:41.199642Z"
        },
        "id": "Ai2e8bBrvNFb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "img_height = 250\n",
        "img_width = 250"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW3ze5SNvNFb"
      },
      "source": [
        "The dataset that we are using has 3 different folders, and each of these have 2 folders within them having a folder for accident images and non accident images. Do look and scroll through them to verify and see the structure.  \n",
        "In order to get our:  \n",
        "1. train,\n",
        "2. test\n",
        "3. and validation split,  \n",
        "\n",
        "we will use keras's inbuilt *image_dataset_from_directory()* function which is able to generate a tf dataset containing the images as well as their corresponding classes from the folder that we pass into the parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "execution": {
          "iopub.execute_input": "2023-04-05T11:54:41.210363Z",
          "iopub.status.busy": "2023-04-05T11:54:41.208925Z",
          "iopub.status.idle": "2023-04-05T11:54:44.141991Z",
          "shell.execute_reply": "2023-04-05T11:54:44.141039Z",
          "shell.execute_reply.started": "2023-04-05T11:54:41.210322Z"
        },
        "id": "OaxnG908vNFc",
        "outputId": "9984d645-05d5-4ef9-9a4a-37894cfe7377",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ece4ea23d4f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m training_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'/kaggle/input/accident-detection-from-cctv-footage/data/train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "training_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/kaggle/input/accident-detection-from-cctv-footage/data/train',\n",
        "    seed=101,\n",
        "    image_size= (img_height, img_width),\n",
        "    batch_size=batch_size\n",
        "\n",
        ")\n",
        "\n",
        "testing_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/kaggle/input/accident-detection-from-cctv-footage/data/test',\n",
        "    seed=101,\n",
        "    image_size= (img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "validation_ds =  tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/kaggle/input/accident-detection-from-cctv-footage/data/val',\n",
        "    seed=101,\n",
        "    image_size= (img_height, img_width),\n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I8vgKeUvNFc"
      },
      "source": [
        "Notice the output reading the files as well as the classes it recognises!  \n",
        "Now, we'll set up a few performace parameters that will enhance runtime training of our model. I've learnt to use this from [this excellent notebook here](https://www.kaggle.com/code/vanvalkenberg/cnn-for-accident-detection-83-val-accuracy/notebook), so do check that out as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "execution": {
          "iopub.execute_input": "2023-04-05T11:54:44.145195Z",
          "iopub.status.busy": "2023-04-05T11:54:44.144793Z",
          "iopub.status.idle": "2023-04-05T11:54:44.152407Z",
          "shell.execute_reply": "2023-04-05T11:54:44.151433Z",
          "shell.execute_reply.started": "2023-04-05T11:54:44.145148Z"
        },
        "id": "rjRvgGGGvNFc",
        "outputId": "009fe4c4-9cae-48e4-8c9a-3bb60df9a4f9",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'training_ds' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-41d7011f0bec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Configuring dataset for performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mAUTOTUNE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_ds' is not defined"
          ]
        }
      ],
      "source": [
        "class_names = training_ds.class_names\n",
        "\n",
        "## Configuring dataset for performance\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "training_ds = training_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "testing_ds = testing_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDciyaEcvNFd"
      },
      "source": [
        "# Defining our Pre-Trained Model  \n",
        "The next step is defining and creating our model. In order to increase accuracy and speed up training process, we'll go ahead and use a pre trained model for this task. Why you may ask?  This is because a pretrained convnet already has a very good idea of what features to look for in an image and can find them very effectively since it hs been trained on millions of images. So, if we can determine the presence of features all the rest of the model needs to do is determine which combination of features makes a specific image.  \n",
        "So all we've to do is:\n",
        "1. Define the base pretrained layer\n",
        "2. Add final few layers that are specific to our function and task to enhance ability in those categories\n",
        "3. Train our model!  \n",
        "Lets use Googles MobileNetV2 for this purpose...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:54:44.15479Z",
          "iopub.status.busy": "2023-04-05T11:54:44.15409Z",
          "iopub.status.idle": "2023-04-05T11:54:45.368398Z",
          "shell.execute_reply": "2023-04-05T11:54:45.367376Z",
          "shell.execute_reply.started": "2023-04-05T11:54:44.154749Z"
        },
        "id": "M1KfT0BjvNFd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "img_shape = (img_height, img_width, 3)\n",
        "\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=img_shape,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIvXjRCEvNFd"
      },
      "source": [
        "Notice how we set trainable to false in order to make sure model wonâ€™t make any changes to the weights of any layers that are already frozen during training.  \n",
        "We also exclude the top of the model since we will perform classification on our own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX7W36ngvNFd"
      },
      "source": [
        "# Creating Final Model  \n",
        "We now go ahead and create our final model which consists of the base model, and 3 more layers for performing convolution. The 2d output of the convolution layer is flattened and fed to a dense output layer to perform the classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:54:45.370439Z",
          "iopub.status.busy": "2023-04-05T11:54:45.37005Z",
          "iopub.status.idle": "2023-04-05T11:54:45.774177Z",
          "shell.execute_reply": "2023-04-05T11:54:45.773014Z",
          "shell.execute_reply.started": "2023-04-05T11:54:45.370402Z"
        },
        "id": "uBVUI-PSvNFe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.Conv2D(128, 3, activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(len(class_names), activation= 'softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:54:45.776107Z",
          "iopub.status.busy": "2023-04-05T11:54:45.775728Z",
          "iopub.status.idle": "2023-04-05T11:54:45.793647Z",
          "shell.execute_reply": "2023-04-05T11:54:45.792501Z",
          "shell.execute_reply.started": "2023-04-05T11:54:45.776069Z"
        },
        "id": "Q-RnulInvNFe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxktN31KvNFe"
      },
      "source": [
        "We'll let our model run for 50 epochs, which seems like a decent enough number. Increasing the epochs should result in an increase in accuracy uptil a certain point only though..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:54:45.7959Z",
          "iopub.status.busy": "2023-04-05T11:54:45.79532Z",
          "iopub.status.idle": "2023-04-05T11:56:42.57015Z",
          "shell.execute_reply": "2023-04-05T11:56:42.569041Z",
          "shell.execute_reply.started": "2023-04-05T11:54:45.795862Z"
        },
        "id": "p422iKopvNFe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "history = model.fit(training_ds, validation_data = validation_ds, epochs = 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:56:42.572425Z",
          "iopub.status.busy": "2023-04-05T11:56:42.571552Z",
          "iopub.status.idle": "2023-04-05T11:56:42.842488Z",
          "shell.execute_reply": "2023-04-05T11:56:42.841506Z",
          "shell.execute_reply.started": "2023-04-05T11:56:42.572384Z"
        },
        "id": "90bgdoVdvNFe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label = 'training loss')\n",
        "plt.plot(history.history['accuracy'], label = 'training accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:56:42.572425Z",
          "iopub.status.busy": "2023-04-05T11:56:42.571552Z",
          "iopub.status.idle": "2023-04-05T11:56:42.842488Z",
          "shell.execute_reply": "2023-04-05T11:56:42.841506Z",
          "shell.execute_reply.started": "2023-04-05T11:56:42.572384Z"
        },
        "id": "B8ki2mo_4Jwi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label = 'training loss')\n",
        "plt.plot(history.history['accuracy'], label = 'training accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:56:42.846938Z",
          "iopub.status.busy": "2023-04-05T11:56:42.846642Z",
          "iopub.status.idle": "2023-04-05T11:56:43.09917Z",
          "shell.execute_reply": "2023-04-05T11:56:43.098165Z",
          "shell.execute_reply.started": "2023-04-05T11:56:42.846909Z"
        },
        "id": "XgXrWREbvNFe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['val_loss'], label = 'validation loss')\n",
        "plt.plot(history.history['val_accuracy'], label = 'validation accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQyFVE_4vNFe"
      },
      "source": [
        "The function below looks a bit complicated, but is a simple helper function which shows the image, the predicted class and the actual class for each image in the test dataset. Run it and have a look at how accurate the model seems and where it seems to be struggling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:56:43.100842Z",
          "iopub.status.busy": "2023-04-05T11:56:43.10047Z",
          "iopub.status.idle": "2023-04-05T11:56:48.80346Z",
          "shell.execute_reply": "2023-04-05T11:56:48.802025Z",
          "shell.execute_reply.started": "2023-04-05T11:56:43.100802Z"
        },
        "id": "vWzB2Gx6vNFf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "AccuracyVector = []\n",
        "plt.figure(figsize=(40, 40))\n",
        "for images, labels in testing_ds.take(1):\n",
        "    predictions = model.predict(images)\n",
        "    predlabel = []\n",
        "    prdlbl = []\n",
        "\n",
        "    for mem in predictions:\n",
        "        predlabel.append(class_names[np.argmax(mem)])\n",
        "        prdlbl.append(np.argmax(mem))\n",
        "\n",
        "    AccuracyVector = np.array(prdlbl) == labels\n",
        "    for i in range(40):\n",
        "        ax = plt.subplot(10, 4, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title('Pred: '+ predlabel[i]+' actl:'+class_names[labels[i]] )\n",
        "        plt.axis('off')\n",
        "        plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T12:12:22.776798Z",
          "iopub.status.busy": "2023-04-05T12:12:22.776067Z",
          "iopub.status.idle": "2023-04-05T12:12:22.81507Z",
          "shell.execute_reply": "2023-04-05T12:12:22.814085Z",
          "shell.execute_reply.started": "2023-04-05T12:12:22.77676Z"
        },
        "id": "e4DzzqTBvNFf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "truePositive=0\n",
        "trueNegative=0\n",
        "falsePositive=0\n",
        "falseNegative=0\n",
        "#positive event is accident negative event is non accident\n",
        "for i in range(0,100):\n",
        "    if(predlabel[i]==class_names[labels[i]] and predlabel[i]=='Accident'):\n",
        "        truePositive+=1\n",
        "    elif(predlabel[i]==class_names[labels[i]] and predlabel[i]=='Non Accident'):\n",
        "        trueNegative+=1\n",
        "    elif(predlabel[i]=='Non Accident' and class_names[labels[i]]=='Accident'):\n",
        "        falseNegative+=1\n",
        "    else:\n",
        "        falsePositive+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T12:16:08.471811Z",
          "iopub.status.busy": "2023-04-05T12:16:08.471439Z",
          "iopub.status.idle": "2023-04-05T12:16:08.47805Z",
          "shell.execute_reply": "2023-04-05T12:16:08.476876Z",
          "shell.execute_reply.started": "2023-04-05T12:16:08.471778Z"
        },
        "id": "ywPqFROFvNFf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(f'True positives are: {truePositive}')\n",
        "print(f'True negatives are: {trueNegative}')\n",
        "print(f'False negatives are: {falseNegative}')\n",
        "print(f'False positives are: {falsePositive}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muEeLSy7vNFf"
      },
      "source": [
        "We can go ahead and view the models layers through the plot_model function below provided by keras for an intuitive view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:56:48.805492Z",
          "iopub.status.busy": "2023-04-05T11:56:48.804927Z",
          "iopub.status.idle": "2023-04-05T11:56:49.514501Z",
          "shell.execute_reply": "2023-04-05T11:56:49.513211Z",
          "shell.execute_reply.started": "2023-04-05T11:56:48.80543Z"
        },
        "id": "1PONOzARvNFf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWQ4iwe9vNFf"
      },
      "source": [
        "And thats all! We've successfully creating a model with an accuracy of around 90%. Notice that this can be further improved by performing image manipulation, performing pooling and training our model for a longer epoch or even adding more layers.. However, for our use case, this model we created is perfectly fine.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:56:49.517204Z",
          "iopub.status.busy": "2023-04-05T11:56:49.516533Z",
          "iopub.status.idle": "2023-04-05T11:56:49.523279Z",
          "shell.execute_reply": "2023-04-05T11:56:49.521976Z",
          "shell.execute_reply.started": "2023-04-05T11:56:49.517156Z"
        },
        "id": "MN7G8oWjvNFf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLx-gaYSvNFf"
      },
      "source": [
        "# Testing Model on Videos  \n",
        "In order to use our model on a video, which is our expected use case of a CCTV footage, we will have to use OpenCV in order get the individual frames.  \n",
        "Lets define a function which takes in each frame and converts it into a tensor and then predicts the output class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:56:49.525533Z",
          "iopub.status.busy": "2023-04-05T11:56:49.524897Z",
          "iopub.status.idle": "2023-04-05T11:56:49.533911Z",
          "shell.execute_reply": "2023-04-05T11:56:49.532937Z",
          "shell.execute_reply.started": "2023-04-05T11:56:49.525495Z"
        },
        "id": "ZpPNh-VxvNFf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def predict_frame(img):\n",
        "    img_array = tf.keras.utils.img_to_array(img)\n",
        "    img_batch = np.expand_dims(img_array, axis=0)\n",
        "    prediction=(model.predict(img_batch) > 0.5).astype(\"int32\")\n",
        "    if(prediction[0][0]==0):\n",
        "        return(\"Accident Detected\")\n",
        "    else:\n",
        "        return(\"No Accident\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQbqwqZfvNFg"
      },
      "source": [
        "The following code below makes use of OpenCV. Firstly, we read the video in and grab every 20th frame(in order to reduce total computation for this demonstration) and then we can resize the image and run our function on it.  \n",
        "We'll store the label and the image in a list which we can easily access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:56:49.536207Z",
          "iopub.status.busy": "2023-04-05T11:56:49.535503Z",
          "iopub.status.idle": "2023-04-05T11:56:56.975669Z",
          "shell.execute_reply": "2023-04-05T11:56:56.974693Z",
          "shell.execute_reply.started": "2023-04-05T11:56:49.536169Z"
        },
        "id": "cCwWCM0IvNFg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "image=[]\n",
        "label=[]\n",
        "\n",
        "c=1\n",
        "cap= cv2.VideoCapture('/kaggle/input/cctvfootagevideo/videoplayback (online-video-cutter.com).mp4')\n",
        "while True:\n",
        "    grabbed, frame = cap.read()\n",
        "    if c%30==0:\n",
        "        print(c)\n",
        "        resized_frame=tf.keras.preprocessing.image.smart_resize(frame, (img_height, img_width), interpolation='bilinear')\n",
        "        image.append(frame)\n",
        "        label.append(predict_frame(resized_frame))\n",
        "        if(len(image)==75):\n",
        "            break\n",
        "    c+=1\n",
        "\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLP0yJZPvNFg"
      },
      "source": [
        "Lets see any random frame and see what the outcome is..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:56:56.97796Z",
          "iopub.status.busy": "2023-04-05T11:56:56.977339Z",
          "iopub.status.idle": "2023-04-05T11:56:57.308453Z",
          "shell.execute_reply": "2023-04-05T11:56:57.30751Z",
          "shell.execute_reply.started": "2023-04-05T11:56:56.977921Z"
        },
        "id": "T2LiS2j9vNFg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(label[20])\n",
        "print(plt.imshow(image[20]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "607C9Ay0vNFg"
      },
      "source": [
        "Looks about right! There seems to be an accident occuring in this frame. Our model generalizes well and can be used for practical applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "munEvQ4RvNFg"
      },
      "source": [
        "# Converting to TFLite Model  \n",
        "While we've made our model, it is true that Tensor Flow models are very large and bulky and not suitable for the small processing powers that a CCTV surveillance system will handle. For this purpose, we'll convert our Tf model into a TFLite model through the API's available by keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:56:57.310674Z",
          "iopub.status.busy": "2023-04-05T11:56:57.310062Z",
          "iopub.status.idle": "2023-04-05T11:57:31.114174Z",
          "shell.execute_reply": "2023-04-05T11:57:31.112918Z",
          "shell.execute_reply.started": "2023-04-05T11:56:57.310622Z"
        },
        "id": "bdKhJhdnvNFg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Convert the model.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('tf_lite_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp3E7zJXvNFg"
      },
      "source": [
        "A TFLite model is referred to as an interpreter. We open it up and have a look at the input and output shape. It should be a single image of height and width 250 by 250 with 3 colour channels.  \n",
        "The output can be of 2 types only. Accident or Non Accident."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:57:31.116198Z",
          "iopub.status.busy": "2023-04-05T11:57:31.115777Z",
          "iopub.status.idle": "2023-04-05T11:57:31.152774Z",
          "shell.execute_reply": "2023-04-05T11:57:31.151731Z",
          "shell.execute_reply.started": "2023-04-05T11:57:31.116153Z"
        },
        "id": "-uzSDAx9vNFg",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM3nXbVQvNFg"
      },
      "source": [
        "While the steps below aren't necessary, I'll still show you incase you have to perform a similair task for a different model where the input tensor might change or be different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:57:31.15559Z",
          "iopub.status.busy": "2023-04-05T11:57:31.15451Z",
          "iopub.status.idle": "2023-04-05T11:57:31.180218Z",
          "shell.execute_reply": "2023-04-05T11:57:31.17843Z",
          "shell.execute_reply.started": "2023-04-05T11:57:31.155547Z"
        },
        "id": "Mll5oxT2vNFh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "interpreter.resize_tensor_input(input_details[0]['index'], (1, 250, 250,3))\n",
        "interpreter.resize_tensor_input(output_details[0]['index'], (1, 2))\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(\"Input Shape:\", input_details[0]['shape'])\n",
        "print(\"Input Type:\", input_details[0]['dtype'])\n",
        "print(\"Output Shape:\", output_details[0]['shape'])\n",
        "print(\"Output Type:\", output_details[0]['dtype'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKmHU4UavNFl"
      },
      "source": [
        "# Trying Our TFLite Model Out  \n",
        "We'll try our TFLite model on a random image and see what our output is and if it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:57:31.183091Z",
          "iopub.status.busy": "2023-04-05T11:57:31.182313Z",
          "iopub.status.idle": "2023-04-05T11:57:31.203991Z",
          "shell.execute_reply": "2023-04-05T11:57:31.203044Z",
          "shell.execute_reply.started": "2023-04-05T11:57:31.183049Z"
        },
        "id": "1oB7rnEEvNFl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "im=Image.open(\"/kaggle/input/accident-detection-from-cctv-footage/data/train/Non Accident/5_17.jpg\").resize((250,250))\n",
        "img_array = tf.keras.utils.img_to_array(im)\n",
        "img_batch = np.expand_dims(img_array, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAvvIAF3vNFl"
      },
      "source": [
        "The below lines are equivalent to performing a prediction in a TF model. *interpretor.get_tensor(*) performs the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-05T11:57:31.205977Z",
          "iopub.status.busy": "2023-04-05T11:57:31.205593Z",
          "iopub.status.idle": "2023-04-05T11:57:31.50701Z",
          "shell.execute_reply": "2023-04-05T11:57:31.506045Z",
          "shell.execute_reply.started": "2023-04-05T11:57:31.205939Z"
        },
        "id": "F0pL6_gDvNFl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "interpreter.set_tensor(input_details[0]['index'], img_batch)\n",
        "interpreter.invoke()\n",
        "tflite_model_predictions = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"Prediction results:\", tflite_model_predictions[0][1])\n",
        "print(plt.imshow(im))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDZyZ-LDvNFl"
      },
      "source": [
        "It works. We've got a complete end to end system for accident detection now that should work very well indeed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
